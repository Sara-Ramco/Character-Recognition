{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load training dataset and testing dataset\n",
    "df = pd.read_csv('./train.csv')\n",
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization, thanks Abdul for sharing the visualization code\n",
    "def plot_char(df):  \n",
    "    word_end_indices = list(df[df.NextId == -1].index)\n",
    "    begin = 0\n",
    "    wordset = set([])\n",
    "    words = []\n",
    "    word_images = []\n",
    "    for wend in word_end_indices:\n",
    "        word = ''.join(df['Prediction'][begin:wend+1])\n",
    "        if word not in wordset:\n",
    "            wordset.add(word)\n",
    "            word_images.append(df.iloc[begin:wend+1,4:].as_matrix())\n",
    "            words.append(word)\n",
    "        begin = wend+1\n",
    "    word_images = np.array(word_images)\n",
    "\n",
    "    num_words = 10\n",
    "    max_len = max(map(len, words))\n",
    "    fig, axes_arr = plt.subplots(num_words, max_len, figsize=(15,16))\n",
    "    for i, w in enumerate(word_images[:num_words]):\n",
    "        w = w.reshape(-1, 16, 8)\n",
    "        for j, letter in enumerate(w):\n",
    "            axes_arr[i,j].imshow(letter)\n",
    "            axes_arr[i,j].set_title(words[i][j])\n",
    "            axes_arr[i,j].axis('off')\n",
    "        for k in range(j,max_len):\n",
    "            axes_arr[i,k].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the unique word list from dataset\n",
    "def word_list(df):\n",
    "    word_end_indices = list(df[df.NextId == -1].index)\n",
    "    words = []\n",
    "    begin =0\n",
    "    for wend in word_end_indices:\n",
    "        word = ''.join(df['Prediction'][begin:wend+1])\n",
    "        begin = wend + 1\n",
    "        words.append(word)\n",
    "    \n",
    "    words = set(words)\n",
    "    return list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Prediction', 'NextId', 'Position', 'p_0_0', 'p_0_1', 'p_0_2',\n",
       "       'p_0_3', 'p_0_4', 'p_0_5',\n",
       "       ...\n",
       "       'p_14_6', 'p_14_7', 'p_15_0', 'p_15_1', 'p_15_2', 'p_15_3', 'p_15_4',\n",
       "       'p_15_5', 'p_15_6', 'p_15_7'],\n",
       "      dtype='object', length=132)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understanding the data first\n",
    "# Show column list of dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['axi',\n",
       " 'uzz',\n",
       " 'abulously',\n",
       " 'ejuvenating',\n",
       " 'eclaring',\n",
       " 'evving',\n",
       " 'earbook',\n",
       " 'mitted',\n",
       " 'nconsequential',\n",
       " 'poiling',\n",
       " 'omparatively',\n",
       " 'ransform',\n",
       " 'olcanic',\n",
       " 'afeteria',\n",
       " 'isqualified',\n",
       " 'mbraces',\n",
       " 'ccountability',\n",
       " 'uzzlement',\n",
       " 'ero',\n",
       " 'obble',\n",
       " 'enu',\n",
       " 'hadow',\n",
       " 'anquish',\n",
       " 'ncomfortably',\n",
       " 'ump',\n",
       " 'ake',\n",
       " 'ugging',\n",
       " 'ormalization',\n",
       " 'rojections',\n",
       " 'ndustrialized',\n",
       " 'ylophone',\n",
       " 'eeker',\n",
       " 'uizzically',\n",
       " 'ommanding',\n",
       " 'eography',\n",
       " 'cknowledgement',\n",
       " 'ate',\n",
       " 'idding',\n",
       " 'acking',\n",
       " 'gonizingly',\n",
       " 'kiing',\n",
       " 'nworkable',\n",
       " 'nnouncing',\n",
       " 'uff',\n",
       " 'ecompress',\n",
       " 'ustifications',\n",
       " 'urrounded',\n",
       " 'nvulnerable',\n",
       " 'pproaching',\n",
       " 'rightfully',\n",
       " 'ympathetically',\n",
       " 'nexpected',\n",
       " 'wab',\n",
       " 'omplex',\n",
       " 'overning']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word list in training set\n",
    "word_list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13ed7f1d0>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13ee80ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at the frequency of each character appears in the trainning set\n",
    "prediction_value_counts = df['Prediction'].value_counts()\n",
    "prediction_value_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.012500107677328122"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the correlation between prediction and position\n",
    "df['Pred_ORD'] = df['Prediction'].apply(lambda x: ord(x))\n",
    "df['Pred_ORD'].corr(df['Position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Pred_ORD'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "# 1. Get the pixel array with size of 128 of next character, denoted as y_next\n",
    "# 2. Get the pixel array with size of 128 of previous character, denoted as y_prev\n",
    "# 3. Get position array with size of 16, denoted as p\n",
    "# 4. Convert Prediction character into an array with size of 26, denoted as y\n",
    "\n",
    "def preprocessing(df):\n",
    "    \n",
    "    y = np.zeros((df.shape[0], 26))\n",
    "    y_next = np.zeros((df.shape[0], 128))\n",
    "    y_prev = np.zeros((df.shape[0], 128))\n",
    "    p = np.zeros((df.shape[0], 16))\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        y_tmp = np.zeros((1, 26))\n",
    "        p_tmp = np.zeros((1, 16))\n",
    "        y_tmp[0, ord(df.iloc[i, 1])-97] = 1\n",
    "        p_tmp[0, df.iloc[i, 3]] = 1\n",
    "        y[i, :] = y_tmp\n",
    "        p[i, :] = p_tmp\n",
    "        if df.iloc[i, 2] != -1:\n",
    "            y_next[i, :] = df.iloc[i+1, 4:]\n",
    "        \n",
    "        if i > 0 and df.iloc[i-1, 2] != -1:\n",
    "            y_prev[i, :] = df.iloc[i-1, 4:]\n",
    "        \n",
    "    return y, y_next, y_prev, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess for trainning dataset\n",
    "y, y_next, y_prev, p = preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess for testing dataset\n",
    "y_t, y_t_next, y_t_prev, p_t = preprocessing(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing step 2\n",
    "# Concatenate previous image data and next image data into current one\n",
    "# Concatenate metadata of position info into the final set\n",
    "\n",
    "# Therefore, the final training data set contains information about \n",
    "# pixels of current character, pixels of next character, pixels of previous character and position\n",
    "\n",
    "def preprocessing_2(X, p, y_next, y_prev):\n",
    "    #X = X.drop(columns =['Id','Prediction', 'NextId', 'Position'])\n",
    "    X = X.drop(['Id','Prediction', 'NextId', 'Position'], axis = 1)\n",
    "    X = X.values\n",
    "    X = np.concatenate((y_prev,X), axis = 1)\n",
    "    X = np.concatenate((X, y_next), axis = 1)\n",
    "    X = np.concatenate((X, p), axis = 1)\n",
    "    X = X.reshape(X.shape[0], 50, 8, 1)\n",
    "    X = X.astype('float32')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set into 2 sets for training and validation, split ratio = 0.2 \n",
    "X_train, X_test, y_train, y_test, p_train, p_test, y_next_train, y_next_test, y_prev_train, y_prev_test = train_test_split(df, y, p, y_next, y_prev, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for training and validation\n",
    "X_train = preprocessing_2(X_train, p_train, y_next_train, y_prev_train)\n",
    "X_test = preprocessing_2(X_test, p_test, y_next_test, y_prev_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data for prediction\n",
    "final_test = preprocessing_2(df_test, p_t, y_t_next, y_t_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN model for training\n",
    "def loadModel():\n",
    "    # Define model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, 3, 3, activation='relu', input_shape = (50, 8, 1)))\n",
    "    #model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, 3, 3, activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation ='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(26, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(50, 8, 1))`\n",
      "  after removing the cwd from sys.path.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33254 samples, validate on 8314 samples\n",
      "Epoch 1/100\n",
      " - 49s - loss: 2.1647 - acc: 0.3731 - val_loss: 1.0440 - val_acc: 0.7190\n",
      "Epoch 2/100\n",
      " - 49s - loss: 1.3329 - acc: 0.5987 - val_loss: 0.6751 - val_acc: 0.8104\n",
      "Epoch 3/100\n",
      " - 48s - loss: 1.0921 - acc: 0.6660 - val_loss: 0.5928 - val_acc: 0.8276\n",
      "Epoch 4/100\n",
      " - 49s - loss: 0.9783 - acc: 0.6957 - val_loss: 0.4720 - val_acc: 0.8655\n",
      "Epoch 5/100\n",
      " - 129s - loss: 0.8841 - acc: 0.7246 - val_loss: 0.4285 - val_acc: 0.8776\n",
      "Epoch 6/100\n",
      " - 50s - loss: 0.8209 - acc: 0.7452 - val_loss: 0.3826 - val_acc: 0.8875\n",
      "Epoch 7/100\n",
      " - 48s - loss: 0.7570 - acc: 0.7644 - val_loss: 0.3489 - val_acc: 0.9023\n",
      "Epoch 8/100\n",
      " - 143s - loss: 0.7221 - acc: 0.7755 - val_loss: 0.3490 - val_acc: 0.8992\n",
      "Epoch 9/100\n",
      " - 58s - loss: 0.6747 - acc: 0.7865 - val_loss: 0.3139 - val_acc: 0.9104\n",
      "Epoch 10/100\n",
      " - 55s - loss: 0.6414 - acc: 0.8002 - val_loss: 0.2985 - val_acc: 0.9144\n",
      "Epoch 11/100\n",
      " - 51s - loss: 0.6075 - acc: 0.8085 - val_loss: 0.2781 - val_acc: 0.9188\n",
      "Epoch 12/100\n",
      " - 49s - loss: 0.5870 - acc: 0.8150 - val_loss: 0.2893 - val_acc: 0.9148\n",
      "Epoch 13/100\n",
      " - 49s - loss: 0.5669 - acc: 0.8187 - val_loss: 0.2669 - val_acc: 0.9231\n",
      "Epoch 14/100\n",
      " - 48s - loss: 0.5465 - acc: 0.8261 - val_loss: 0.2702 - val_acc: 0.9211\n",
      "Epoch 15/100\n",
      " - 48s - loss: 0.5437 - acc: 0.8254 - val_loss: 0.2465 - val_acc: 0.9259\n",
      "Epoch 16/100\n",
      " - 49s - loss: 0.5149 - acc: 0.8327 - val_loss: 0.2378 - val_acc: 0.9295\n",
      "Epoch 17/100\n",
      " - 48s - loss: 0.5038 - acc: 0.8378 - val_loss: 0.2436 - val_acc: 0.9281\n",
      "Epoch 18/100\n",
      " - 49s - loss: 0.4943 - acc: 0.8434 - val_loss: 0.2393 - val_acc: 0.9281\n",
      "Epoch 19/100\n",
      " - 48s - loss: 0.4763 - acc: 0.8475 - val_loss: 0.2315 - val_acc: 0.9311\n",
      "Epoch 20/100\n",
      " - 48s - loss: 0.4539 - acc: 0.8543 - val_loss: 0.2254 - val_acc: 0.9324\n",
      "Epoch 21/100\n",
      " - 49s - loss: 0.4407 - acc: 0.8557 - val_loss: 0.2258 - val_acc: 0.9329\n",
      "Epoch 22/100\n",
      " - 48s - loss: 0.4150 - acc: 0.8638 - val_loss: 0.2180 - val_acc: 0.9361\n",
      "Epoch 23/100\n",
      " - 49s - loss: 0.4207 - acc: 0.8643 - val_loss: 0.2068 - val_acc: 0.9377\n",
      "Epoch 24/100\n",
      " - 48s - loss: 0.4079 - acc: 0.8661 - val_loss: 0.2072 - val_acc: 0.9377\n",
      "Epoch 25/100\n",
      " - 48s - loss: 0.3976 - acc: 0.8712 - val_loss: 0.2084 - val_acc: 0.9383\n",
      "Epoch 26/100\n",
      " - 48s - loss: 0.3975 - acc: 0.8688 - val_loss: 0.2046 - val_acc: 0.9360\n",
      "Epoch 27/100\n",
      " - 48s - loss: 0.3735 - acc: 0.8782 - val_loss: 0.2007 - val_acc: 0.9413\n",
      "Epoch 28/100\n",
      " - 49s - loss: 0.3676 - acc: 0.8804 - val_loss: 0.1982 - val_acc: 0.9415\n",
      "Epoch 29/100\n",
      " - 48s - loss: 0.3589 - acc: 0.8831 - val_loss: 0.1985 - val_acc: 0.9414\n",
      "Epoch 30/100\n",
      " - 48s - loss: 0.3480 - acc: 0.8831 - val_loss: 0.1933 - val_acc: 0.9431\n",
      "Epoch 31/100\n",
      " - 48s - loss: 0.3386 - acc: 0.8872 - val_loss: 0.1851 - val_acc: 0.9446\n",
      "Epoch 32/100\n",
      " - 48s - loss: 0.3218 - acc: 0.8935 - val_loss: 0.1937 - val_acc: 0.9417\n",
      "Epoch 33/100\n",
      " - 49s - loss: 0.3144 - acc: 0.8963 - val_loss: 0.1830 - val_acc: 0.9455\n",
      "Epoch 34/100\n",
      " - 48s - loss: 0.3010 - acc: 0.9001 - val_loss: 0.1814 - val_acc: 0.9474\n",
      "Epoch 35/100\n",
      " - 49s - loss: 0.3035 - acc: 0.8994 - val_loss: 0.1804 - val_acc: 0.9459\n",
      "Epoch 36/100\n",
      " - 48s - loss: 0.2828 - acc: 0.9052 - val_loss: 0.1809 - val_acc: 0.9465\n",
      "Epoch 37/100\n",
      " - 48s - loss: 0.2812 - acc: 0.9052 - val_loss: 0.1833 - val_acc: 0.9467\n",
      "Epoch 38/100\n",
      " - 48s - loss: 0.2794 - acc: 0.9066 - val_loss: 0.1758 - val_acc: 0.9496\n",
      "Epoch 39/100\n",
      " - 48s - loss: 0.2784 - acc: 0.9062 - val_loss: 0.1738 - val_acc: 0.9501\n",
      "Epoch 40/100\n",
      " - 48s - loss: 0.2752 - acc: 0.9052 - val_loss: 0.1714 - val_acc: 0.9501\n",
      "Epoch 41/100\n",
      " - 48s - loss: 0.2598 - acc: 0.9114 - val_loss: 0.1696 - val_acc: 0.9497\n",
      "Epoch 42/100\n",
      " - 48s - loss: 0.2651 - acc: 0.9095 - val_loss: 0.1728 - val_acc: 0.9489\n",
      "Epoch 43/100\n",
      " - 49s - loss: 0.2569 - acc: 0.9132 - val_loss: 0.1797 - val_acc: 0.9477\n",
      "Epoch 44/100\n",
      " - 48s - loss: 0.2509 - acc: 0.9138 - val_loss: 0.1818 - val_acc: 0.9492\n",
      "Epoch 45/100\n",
      " - 48s - loss: 0.2511 - acc: 0.9145 - val_loss: 0.1718 - val_acc: 0.9516\n",
      "Epoch 46/100\n",
      " - 48s - loss: 0.2501 - acc: 0.9158 - val_loss: 0.1765 - val_acc: 0.9486\n",
      "Epoch 47/100\n",
      " - 48s - loss: 0.2392 - acc: 0.9190 - val_loss: 0.1735 - val_acc: 0.9514\n",
      "Epoch 48/100\n",
      " - 48s - loss: 0.2387 - acc: 0.9186 - val_loss: 0.1777 - val_acc: 0.9486\n",
      "Epoch 49/100\n",
      " - 48s - loss: 0.2273 - acc: 0.9226 - val_loss: 0.1694 - val_acc: 0.9504\n",
      "Epoch 50/100\n",
      " - 48s - loss: 0.2291 - acc: 0.9219 - val_loss: 0.1826 - val_acc: 0.9482\n",
      "Epoch 51/100\n",
      " - 48s - loss: 0.2274 - acc: 0.9225 - val_loss: 0.1757 - val_acc: 0.9502\n",
      "Epoch 52/100\n",
      " - 48s - loss: 0.2135 - acc: 0.9288 - val_loss: 0.1657 - val_acc: 0.9509\n",
      "Epoch 53/100\n",
      " - 48s - loss: 0.2102 - acc: 0.9287 - val_loss: 0.1716 - val_acc: 0.9516\n",
      "Epoch 54/100\n",
      " - 48s - loss: 0.2064 - acc: 0.9279 - val_loss: 0.1745 - val_acc: 0.9521\n",
      "Epoch 55/100\n",
      " - 48s - loss: 0.2090 - acc: 0.9269 - val_loss: 0.1780 - val_acc: 0.9490\n",
      "Epoch 56/100\n",
      " - 48s - loss: 0.2077 - acc: 0.9282 - val_loss: 0.1702 - val_acc: 0.9508\n",
      "Epoch 57/100\n",
      " - 48s - loss: 0.2007 - acc: 0.9317 - val_loss: 0.1700 - val_acc: 0.9527\n",
      "Epoch 58/100\n",
      " - 49s - loss: 0.1894 - acc: 0.9350 - val_loss: 0.1665 - val_acc: 0.9522\n",
      "Epoch 59/100\n",
      " - 48s - loss: 0.1942 - acc: 0.9345 - val_loss: 0.1689 - val_acc: 0.9544\n",
      "Epoch 60/100\n",
      " - 48s - loss: 0.1828 - acc: 0.9367 - val_loss: 0.1668 - val_acc: 0.9537\n",
      "Epoch 61/100\n",
      " - 48s - loss: 0.1792 - acc: 0.9375 - val_loss: 0.1643 - val_acc: 0.9566\n",
      "Epoch 62/100\n",
      " - 48s - loss: 0.1838 - acc: 0.9386 - val_loss: 0.1606 - val_acc: 0.9554\n",
      "Epoch 63/100\n",
      " - 48s - loss: 0.1780 - acc: 0.9388 - val_loss: 0.1712 - val_acc: 0.9545\n",
      "Epoch 64/100\n",
      " - 48s - loss: 0.1754 - acc: 0.9413 - val_loss: 0.1814 - val_acc: 0.9531\n",
      "Epoch 65/100\n",
      " - 48s - loss: 0.1626 - acc: 0.9446 - val_loss: 0.1709 - val_acc: 0.9550\n",
      "Epoch 66/100\n",
      " - 48s - loss: 0.1609 - acc: 0.9442 - val_loss: 0.1647 - val_acc: 0.9560\n",
      "Epoch 67/100\n",
      " - 48s - loss: 0.1603 - acc: 0.9442 - val_loss: 0.1712 - val_acc: 0.9544\n",
      "Epoch 68/100\n",
      " - 49s - loss: 0.1597 - acc: 0.9447 - val_loss: 0.1636 - val_acc: 0.9572\n",
      "Epoch 69/100\n",
      " - 48s - loss: 0.1594 - acc: 0.9440 - val_loss: 0.1597 - val_acc: 0.9553\n",
      "Epoch 70/100\n",
      " - 49s - loss: 0.1565 - acc: 0.9462 - val_loss: 0.1627 - val_acc: 0.9559\n",
      "Epoch 71/100\n",
      " - 48s - loss: 0.1542 - acc: 0.9474 - val_loss: 0.1641 - val_acc: 0.9580\n",
      "Epoch 72/100\n",
      " - 48s - loss: 0.1507 - acc: 0.9480 - val_loss: 0.1558 - val_acc: 0.9583\n",
      "Epoch 73/100\n",
      " - 48s - loss: 0.1557 - acc: 0.9474 - val_loss: 0.1511 - val_acc: 0.9581\n",
      "Epoch 74/100\n",
      " - 48s - loss: 0.1442 - acc: 0.9506 - val_loss: 0.1528 - val_acc: 0.9601\n",
      "Epoch 75/100\n",
      " - 51s - loss: 0.1431 - acc: 0.9515 - val_loss: 0.1654 - val_acc: 0.9551\n",
      "Epoch 76/100\n",
      " - 48s - loss: 0.1375 - acc: 0.9531 - val_loss: 0.1538 - val_acc: 0.9599\n",
      "Epoch 77/100\n",
      " - 49s - loss: 0.1330 - acc: 0.9528 - val_loss: 0.1724 - val_acc: 0.9555\n",
      "Epoch 78/100\n",
      " - 48s - loss: 0.1404 - acc: 0.9520 - val_loss: 0.1605 - val_acc: 0.9566\n",
      "Epoch 79/100\n",
      " - 48s - loss: 0.1283 - acc: 0.9573 - val_loss: 0.1627 - val_acc: 0.9568\n",
      "Epoch 80/100\n",
      " - 48s - loss: 0.1307 - acc: 0.9564 - val_loss: 0.1680 - val_acc: 0.9574\n",
      "Epoch 81/100\n",
      " - 48s - loss: 0.1260 - acc: 0.9572 - val_loss: 0.1540 - val_acc: 0.9591\n",
      "Epoch 82/100\n",
      " - 49s - loss: 0.1276 - acc: 0.9567 - val_loss: 0.1538 - val_acc: 0.9601\n",
      "Epoch 83/100\n",
      " - 48s - loss: 0.1232 - acc: 0.9589 - val_loss: 0.1603 - val_acc: 0.9583\n",
      "Epoch 84/100\n",
      " - 48s - loss: 0.1217 - acc: 0.9590 - val_loss: 0.1555 - val_acc: 0.9591\n",
      "Epoch 85/100\n",
      " - 50s - loss: 0.1165 - acc: 0.9590 - val_loss: 0.1603 - val_acc: 0.9584\n",
      "Epoch 86/100\n",
      " - 48s - loss: 0.1119 - acc: 0.9617 - val_loss: 0.1644 - val_acc: 0.9575\n",
      "Epoch 87/100\n",
      " - 49s - loss: 0.1159 - acc: 0.9599 - val_loss: 0.1655 - val_acc: 0.9592\n",
      "Epoch 88/100\n",
      " - 49s - loss: 0.1070 - acc: 0.9629 - val_loss: 0.1565 - val_acc: 0.9596\n",
      "Epoch 89/100\n",
      " - 51s - loss: 0.1106 - acc: 0.9618 - val_loss: 0.1582 - val_acc: 0.9602\n",
      "Epoch 90/100\n",
      " - 51s - loss: 0.1140 - acc: 0.9609 - val_loss: 0.1569 - val_acc: 0.9604\n",
      "Epoch 91/100\n",
      " - 48s - loss: 0.1098 - acc: 0.9617 - val_loss: 0.1589 - val_acc: 0.9605\n",
      "Epoch 92/100\n",
      " - 48s - loss: 0.1071 - acc: 0.9635 - val_loss: 0.1638 - val_acc: 0.9599\n",
      "Epoch 93/100\n",
      " - 48s - loss: 0.1049 - acc: 0.9642 - val_loss: 0.1642 - val_acc: 0.9590\n",
      "Epoch 94/100\n",
      " - 48s - loss: 0.1043 - acc: 0.9644 - val_loss: 0.1674 - val_acc: 0.9573\n",
      "Epoch 95/100\n",
      " - 48s - loss: 0.1027 - acc: 0.9640 - val_loss: 0.1637 - val_acc: 0.9593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      " - 48s - loss: 0.1012 - acc: 0.9652 - val_loss: 0.1589 - val_acc: 0.9589\n",
      "Epoch 97/100\n",
      " - 49s - loss: 0.1005 - acc: 0.9657 - val_loss: 0.1683 - val_acc: 0.9602\n",
      "Epoch 98/100\n",
      " - 52s - loss: 0.0964 - acc: 0.9669 - val_loss: 0.1591 - val_acc: 0.9610\n",
      "Epoch 99/100\n",
      " - 49s - loss: 0.0989 - acc: 0.9654 - val_loss: 0.1748 - val_acc: 0.9584\n",
      "Epoch 100/100\n",
      " - 49s - loss: 0.1011 - acc: 0.9662 - val_loss: 0.1598 - val_acc: 0.9601\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = loadModel()\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size =400, epochs=100, verbose = 2, validation_data = (X_test, y_test))\n",
    "#score = model.evaluate(X_test, y_test, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test data with the model\n",
    "y_labels = model.predict(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert labels into characters\n",
    "def get_labels(y_labels):\n",
    "    list_result = y_labels.tolist()\n",
    "    list_char = []\n",
    "    for k in range(len(list_result)):\n",
    "        pred_char = chr(list_result[k].index(max(list_result[k])) + 97)\n",
    "        list_char.append(pred_char)\n",
    "    return list_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset again\n",
    "df_t = pd.read_csv('./test.csv')\n",
    "df_t['Prediction'] = get_labels(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call word_list function to get list of words which appear in the training set\n",
    "words = word_list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "# Function to find similar word in the word list\n",
    "def find_similar_word(test_word, words):\n",
    "    word_list = []\n",
    "    \n",
    "    for word in words:\n",
    "        if len(word) == len(test_word):\n",
    "            word_list.append(word)\n",
    "            \n",
    "    matched_words = difflib.get_close_matches(test_word, word_list)\n",
    "    \n",
    "    if len(matched_words)>0:\n",
    "        return matched_words[0]\n",
    "    else:\n",
    "        return test_word\n",
    "    \n",
    "# Define the function to correct the test word to the word in word dictionary if it is inside the dictionary\n",
    "def correction(df, words):\n",
    "    word_end_indices = list(df[df.NextId == -1].index)\n",
    "    begin =0\n",
    "    for wend in word_end_indices:\n",
    "        test_word = ''.join(df['Prediction'][begin:wend+1])\n",
    "        #if word not in wordset:\n",
    "        #matched_words = difflib.get_close_matches(test_word, words)\n",
    "        \n",
    "        final_word = find_similar_word(test_word, words)\n",
    "        \n",
    "        for i in range(begin, wend+1):\n",
    "            final_word = list(final_word)\n",
    "            df.iloc[i, 1] = final_word[i - begin]\n",
    "        begin = wend + 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call the correction function \n",
    "df_t_new = correction(df_t, words)\n",
    "\n",
    "# Save it to CSV file\n",
    "df_t_new.to_csv('ZhengShuangyue-A0178479E.csv', columns = [\"Prediction\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
